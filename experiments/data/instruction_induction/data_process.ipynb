{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Instruction Induction dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "all_datasets = 'active_to_passive antonyms auto_categorization auto_debugging cause_and_effect common_concept diff first_word_letter informal_to_formal larger_animal letters_list negation num_to_verbal odd_one_out object_counting orthography_starts_with periodic_elements rhymes second_word_letter sentence_similarity sentiment singular_to_plural sum synonyms taxonomy_animal translation_en-de translation_en-es translation_en-fr word_sorting word_unscrambling'\n",
    "all_datasets = all_datasets.split()\n",
    "new_taskes = []\n",
    "for dataset_ in all_datasets:\n",
    "\twith open('./raw/induce/{}.json'.format(dataset_), 'r') as f:\n",
    "\t\tdata = json.load(f)\n",
    "\t\texamples = data['examples']\n",
    "\twith open('./raw/execute/{}.json'.format(dataset_), 'r') as f:\n",
    "\t\tdata_test = json.load(f)\n",
    "\tif len(examples) <= 100:\n",
    "\t\tprint(dataset_)\n",
    "\t\tcontinue\n",
    "\tnew_task_name = f\"{dataset_}_v1\"\n",
    "\t# divide the examples to train and valid where valid is 20% of the examples\n",
    "\tinduce_size = int(0.8 * len(examples))\n",
    "\tvalid_size = len(examples) - induce_size\n",
    "\t# Induce\n",
    "\timport copy\n",
    "\tdata_train = copy.deepcopy(data)\n",
    "\tdata_valid = copy.deepcopy(data)\n",
    "\tdata_train['examples'] = {k: v for k, v in data['examples'].items() if int(k) <= induce_size}\n",
    "\tdata_valid['examples'] = {str(int(k) - induce_size): v for k, v in data['examples'].items() if int(k) > induce_size}\n",
    "\tdata_train['metadata']['num_examples'] = len(data_train['examples'])\n",
    "\tdata_valid['metadata']['num_examples'] = len(data_valid['examples'])\n",
    "\twith open('./raw/induce/{}.json'.format(new_task_name), 'w') as f:\n",
    "\t\tjson.dump(data_train, f, indent=4)\n",
    "\twith open('./raw/valid/{}.json'.format(new_task_name), 'w') as f:\n",
    "\t\tjson.dump(data_valid, f, indent=4)\n",
    "\twith open('./raw/execute/{}.json'.format(new_task_name), 'w') as f:\n",
    "\t\tjson.dump(data_test, f, indent=4)\n",
    "\tnew_taskes += [new_task_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Noisy dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks = ['sentence_similarity'] # Replace with all tasks\n",
    "noisy_sizes = [10, 20, 30, 40, 50, 60, 70, 80, 90]\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "induce_size = 100\n",
    "# noisy_size = 50\n",
    "induce = True\n",
    "induce_data_path = 'raw/induce/'\n",
    "eval_data_path = 'raw/execute/'\n",
    "\n",
    "for noisy_size in noisy_sizes:\n",
    "\tfor task in tasks:\n",
    "\t\twith open('raw/induce/{}.json'.format(task), 'r') as f:\n",
    "\t\t\tdata = json.load(f)\n",
    "\n",
    "\t\t# Induce\n",
    "\t\tnp.random.seed(0)\n",
    "\t\tnoisy_indices = np.random.randint(induce_size-noisy_size, induce_size, size=noisy_size)\n",
    "\n",
    "\t\tout = {}\n",
    "\t\tfor i in range(induce_size):\n",
    "\t\t\tif i < induce_size - noisy_size:\n",
    "\t\t\t\t# Clean\n",
    "\t\t\t\tout[str(i+1)] = {\n",
    "\t\t\t\t\t'input': data['examples'][str(i+1)]['input'],\n",
    "\t\t\t\t\t'output': data['examples'][str(i+1)]['output'],\n",
    "\t\t\t\t}\n",
    "\t\t\telse:\n",
    "\t\t\t\t# Noisy\n",
    "\t\t\t\tout[str(i+1)] = {\n",
    "\t\t\t\t\t'input': data['examples'][str(i+1)]['input'],\n",
    "\t\t\t\t\t'output': data['examples'][str(noisy_indices[i-induce_size+noisy_size])]['output'],\n",
    "\t\t\t\t}\n",
    "\n",
    "\t\tdict = {\"metadata\": {\n",
    "\t\t\t\"num_examples\": len(out)\n",
    "\t\t\t},\n",
    "\t\t\t\t\"examples\": out\n",
    "\t\t}\n",
    "\t\twith open('raw/induce/{}_{}_noisy.json'.format(task, str(noisy_size)), 'x') as fp:\n",
    "\t\t\tjson.dump(dict, fp, indent=4)\n",
    "\t\t\t\n",
    "\t\t# Valid\n",
    "\t\tvalid_size = 100\n",
    "\n",
    "\t\tout = {}\n",
    "\t\tfor i in range(valid_size):\n",
    "\t\t\tout[str(i+1)] = {\n",
    "\t\t\t\t'input': data['examples'][str(i+induce_size+1)]['input'],\n",
    "\t\t\t\t'output': data['examples'][str(i+induce_size+1)]['output'],\n",
    "\t\t\t}\n",
    "\t\t\t\n",
    "\t\tdict = {\"metadata\": {\n",
    "\t\t\t\"num_examples\": len(out)\n",
    "\t\t\t},\n",
    "\t\t\t\t\"examples\": out\n",
    "\t\t}\n",
    "\t\twith open('raw/valid/{}_{}_noisy.json'.format(task, str(noisy_size)), 'x') as fp:\n",
    "\t\t\tjson.dump(dict, fp, indent=4)\n",
    "\t\t\t\n",
    "\t\t# Execute\n",
    "\t\twith open('raw/execute/{}.json'.format(task), 'r') as f:\n",
    "\t\t\tdata = json.load(f)\n",
    "\n",
    "\t\twith open('raw/execute/{}_{}_noisy.json'.format(task, str(noisy_size)), 'x') as fp:\n",
    "\t\t\tjson.dump(data, fp, indent=4)\n",
    "\t\t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "a=-4\n",
    "b=6\n",
    "\n",
    "def func(x1):\n",
    "\tans = a*x1 + b\n",
    "\tprint('Input: {}\\nOutput: {}\\n'.format(x1, ans))\n",
    "\treturn ans\n",
    "\n",
    "induce_size = 200\n",
    "test_size = 100\n",
    "task = 'linear_4'\n",
    "induce = True\n",
    "induce_data_path = 'raw/induce/'\n",
    "eval_data_path = 'raw/execute/'\n",
    "\n",
    "# Induce\n",
    "np.random.seed(0)\n",
    "inputs = np.random.randint(0, 500, induce_size)\n",
    "\n",
    "out = {}\n",
    "for i, j in enumerate(range(induce_size)):\n",
    "\tout[str(i+1)] = {\n",
    "\t\t'input': '{}'.format(inputs[i]),\n",
    "\t\t'output': '{}'.format(func(inputs[i])),\n",
    "\t}x\n",
    "\t\n",
    "dict = {\"metadata\": {\n",
    "\t\"num_examples\": len(out)\n",
    "\t},\n",
    "\t\t\"examples\": out\n",
    "}\n",
    "with open('raw/induce/{}.json'.format(task), 'x') as fp:\n",
    "\tjson.dump(dict, fp, indent=4)\n",
    "\t\n",
    "# Execute\n",
    "np.random.seed(1)\n",
    "inputs = np.random.randint(0, 500, test_size)\n",
    "\n",
    "out = {}\n",
    "for i, j in enumerate(range(test_size)):\n",
    "\tout[str(i+1)] = {\n",
    "\t\t'input': '{}'.format(inputs[i]),\n",
    "\t\t'output': '{}'.format(func(inputs[i])),\n",
    "\t}\n",
    "\t\n",
    "dict = {\"metadata\": {\n",
    "\t\"num_examples\": len(out)\n",
    "\t},\n",
    "\t\t\"examples\": out\n",
    "}\n",
    "with open('raw/execute/{}.json'.format(task), 'x') as fp:\n",
    "\tjson.dump(dict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOISY\n",
    "\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "a=-4\n",
    "b=6\n",
    "\n",
    "c=5\n",
    "d=-8\n",
    "\n",
    "def func(x1):\n",
    "\tans = a*x1 + b\n",
    "\tprint('Input: {}\\nOutput: {}\\n'.format(x1, ans))\n",
    "\treturn ans\n",
    "\n",
    "def noisy_func(x1):\n",
    "\tans = c*x1 + d\n",
    "\tprint('Input: {}\\nOutput: {}\\n'.format(x1, ans))\n",
    "\treturn ans\n",
    "\n",
    "# noisy_sizes = [10, 20, 30, 40, 60, 70, 80, 90]\n",
    "noisy_sizes = [0]\n",
    "\n",
    "for noisy_size in noisy_sizes:\n",
    "\tinduce_size = 100\n",
    "\t# noisy_size = 50\n",
    "\tvalid_size = 100\n",
    "\ttest_size = 100\n",
    "\ttask = 'linear_4_{}_noisy'.format(noisy_size)\n",
    "\tinduce = True\n",
    "\tinduce_data_path = 'raw/induce/'\n",
    "\teval_data_path = 'raw/execute/'\n",
    "\n",
    "\t# Induce\n",
    "\tnp.random.seed(0)\n",
    "\tinputs = np.random.randint(0, 500, induce_size)\n",
    "\n",
    "\tout = {}\n",
    "\tfor i, j in enumerate(range(induce_size)):\n",
    "\t\tout[str(i+1)] = {\n",
    "\t\t\t'input': '{}'.format(inputs[i]),\n",
    "\t\t\t'output': '{}'.format(func(inputs[i]) if i < induce_size - noisy_size else noisy_func(inputs[i])),\n",
    "\t\t}\n",
    "\t\t\n",
    "\tdict = {\"metadata\": {\n",
    "\t\t\"num_examples\": len(out)\n",
    "\t\t},\n",
    "\t\t\t\"examples\": out\n",
    "\t}\n",
    "\twith open('raw/induce/{}.json'.format(task), 'x') as fp:\n",
    "\t\tjson.dump(dict, fp, indent=4)\n",
    "\t\t\n",
    "\t# Valid\n",
    "\tnp.random.seed(1)\n",
    "\tinputs = np.random.randint(0, 500, valid_size)\n",
    "\n",
    "\tout = {}\n",
    "\tfor i, j in enumerate(range(valid_size)):\n",
    "\t\tout[str(i+1)] = {\n",
    "\t\t\t'input': '{}'.format(inputs[i]),\n",
    "\t\t\t'output': '{}'.format(func(inputs[i])),\n",
    "\t\t}\n",
    "\t\t\n",
    "\tdict = {\"metadata\": {\n",
    "\t\t\"num_examples\": len(out)\n",
    "\t\t},\n",
    "\t\t\t\"examples\": out\n",
    "\t}\n",
    "\twith open('raw/valid/{}.json'.format(task), 'x') as fp:\n",
    "\t\tjson.dump(dict, fp, indent=4)\n",
    "\t\t\n",
    "\t# Execute\n",
    "\tnp.random.seed(2)\n",
    "\tinputs = np.random.randint(0, 500, test_size)\n",
    "\n",
    "\tout = {}\n",
    "\tfor i, j in enumerate(range(test_size)):\n",
    "\t\tout[str(i+1)] = {\n",
    "\t\t\t'input': '{}'.format(inputs[i]),\n",
    "\t\t\t'output': '{}'.format(func(inputs[i])),\n",
    "\t\t}\n",
    "\t\t\n",
    "\tdict = {\"metadata\": {\n",
    "\t\t\"num_examples\": len(out)\n",
    "\t\t},\n",
    "\t\t\t\"examples\": out\n",
    "\t}\n",
    "\twith open('raw/execute/{}.json'.format(task), 'x') as fp:\n",
    "\t\tjson.dump(dict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pig-latin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everyay ouyay oxfay, Ethay umpsjay Overyay, ethay Azylay ogday!\n"
     ]
    }
   ],
   "source": [
    "def pig_latin_translator(original_sentence, end1='yay', end2='ay'):\n",
    "    words = original_sentence.split()\n",
    "    vowels = [\"a\", \"e\", \"i\", \"o\", \"u\", \"A\", \"E\", \"I\", \"O\", \"U\"]\n",
    "    output = \"\"\n",
    "    for word in words:\n",
    "        # check if the word is capitalized\n",
    "        capitalized = word[0].isupper()\n",
    "        punctuation = \"\"\n",
    "        if word[-1] in [\".\", \",\", \"!\", \"?\"]:\n",
    "            punctuation = word[-1]\n",
    "            word = word[:-1]\n",
    "        if word[0] in vowels:\n",
    "            if word[-1] == end1[0]:\n",
    "                translated_word = word + end1[1:]\n",
    "            else:\n",
    "                translated_word = word + f\"{end1}\"\n",
    "        else:\n",
    "            start = 0\n",
    "            for i, letter in enumerate(word):\n",
    "                if letter in vowels:\n",
    "                    break\n",
    "                else:\n",
    "                    # if letter == \"y\":\n",
    "                    #     break\n",
    "                    # else:\n",
    "                    start += 1\n",
    "            translated_word = word[start:] + word[:start] + f\"{end2}\"\n",
    "        if capitalized:\n",
    "            translated_word = translated_word.capitalize()\n",
    "        translated_word += punctuation\n",
    "        output += translated_word + \" \"\n",
    "    return output.strip()\n",
    "\n",
    "print(pig_latin_translator(\"Every you fox, The jumps Over, the Lazy dog!\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "task_name = 'pig_latin_variant2_encode'\n",
    "\n",
    "with open('big-bench/pig_latin_encode.json'.format(task_name), 'r') as f:\n",
    "\tdata = json.load(f)\n",
    "\texamples = data['examples']\n",
    "for i in range(len(examples)):\n",
    "\texamples[i]['target'] = pig_latin_translator(examples[i]['input'], end1='ay', end2='ay')\n",
    "\t\n",
    "# noisy_sizes = [10, 20, 30, 40, 60, 70, 80, 90]\n",
    "noisy_sizes = [10, 30, 50, 70, 90]\n",
    "\n",
    "for noisy_size in noisy_sizes:\n",
    "\tinduce_size = 100 if noisy_size > 0 else 200\n",
    "\tvalid_size = 100 if noisy_size > 0 else 0\n",
    "\ttest_size = 100\n",
    "\ttask = '{}_{}_noisy'.format(task_name, noisy_size) if noisy_size > 0 else task_name\n",
    "\n",
    "\t# Induce\n",
    "\tnp.random.seed(0)\n",
    "\tnoisy_indices = np.random.randint(induce_size-noisy_size, induce_size, size=noisy_size)\n",
    "\n",
    "\t# Induce\n",
    "\tout = {}\n",
    "\tfor i, j in enumerate(range(induce_size)):\n",
    "\t\tout[str(i+1)] = {\n",
    "\t\t\t'input': '{}'.format(examples[i]['input']),\n",
    "\t\t\t'output': '{}'.format(examples[i]['target'] if i < induce_size - noisy_size else '{}'.format(examples[i]['input'])),\n",
    "\t\t}\n",
    "\t\t\n",
    "\tdict = {\"metadata\": {\n",
    "\t\t\"num_examples\": len(out),\n",
    "\t\t\"description\": data['description'],\n",
    "\t\t\"task_prefix\": data['task_prefix'],\n",
    "\t\t},\n",
    "\t\t\t\"examples\": out\n",
    "\t}\n",
    "\twith open('raw/induce/{}.json'.format(task), 'w') as fp:\n",
    "\t\tjson.dump(dict, fp, indent=4)\n",
    "\t\t\n",
    "\t# Valid\n",
    "\tif valid_size > 0:\n",
    "\t\tout = {}\n",
    "\t\tfor i, j in enumerate(range(valid_size)):\n",
    "\t\t\tout[str(i+1)] = {\n",
    "\t\t\t\t'input': '{}'.format(examples[induce_size+i]['input']),\n",
    "\t\t\t\t'output': '{}'.format(examples[induce_size+i]['target']),\n",
    "\t\t\t}\n",
    "\t\t\t\n",
    "\t\tdict = {\"metadata\": {\n",
    "\t\t\t\"num_examples\": len(out),\n",
    "\t\t\t\"description\": data['description'],\n",
    "\t\t\t\"task_prefix\": data['task_prefix'],\n",
    "\t\t\t},\n",
    "\t\t\t\t\"examples\": out\n",
    "\t\t}\n",
    "\t\twith open('raw/valid/{}.json'.format(task), 'w') as fp:\n",
    "\t\t\tjson.dump(dict, fp, indent=4)\n",
    "\t\t\n",
    "\t# Execute\n",
    "\tout = {}\n",
    "\tfor i, j in enumerate(range(test_size)):\n",
    "\t\tout[str(i+1)] = {\n",
    "\t\t\t'input': '{}'.format(examples[induce_size+valid_size+i]['input']),\n",
    "\t\t\t'output': '{}'.format(examples[induce_size+valid_size+i]['target']),\n",
    "\t\t}\n",
    "\t\t\n",
    "\tdict = {\"metadata\": {\n",
    "\t\t\"num_examples\": len(out),\n",
    "\t\t\"description\": data['description'],\n",
    "\t\t\"task_prefix\": data['task_prefix'],\n",
    "\t\t},\n",
    "\t\t\t\"examples\": out\n",
    "\t}\n",
    "\twith open('raw/execute/{}.json'.format(task), 'w') as fp:\n",
    "\t\tjson.dump(dict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AG News Remap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ag news redirect1 with text label\n",
    "\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "task_name='ag_news_textlabel_redirect1'\n",
    "\n",
    "induce_size = 100 \n",
    "valid_size = 100 \n",
    "test_size = 100\n",
    "label_map = {0:1, 1:2, 2:3, 3:0}\n",
    "label_text = {0: 'World', 1: 'Sports', 2: 'Business', 3: 'Sci/Tech'}\n",
    "\n",
    "noise_ratio = [10, 30, 50, 70, 90]\n",
    "for noisy_size in noise_ratio:\n",
    "\ttask = '{}_{}_noisy'.format(task_name, noisy_size)\n",
    "\n",
    "\t# shuffle the examples\n",
    "\tnp.random.seed(0)\n",
    "\tperm = np.random.permutation(len(dataset['train']['text']))\n",
    "\texamples = []\n",
    "\tfor i, j in enumerate(perm[:induce_size+valid_size+test_size]):\n",
    "\t\texamples.append({'input':dataset['train']['text'][j], 'target':label_text[label_map[dataset['train']['label'][j]]]})\n",
    "\tprint(\"End\")\n",
    "\tnp.random.seed(0)\n",
    "\tnoisy_indices = np.random.randint(induce_size-noisy_size, induce_size, size=noisy_size)\n",
    "\t# Induce\n",
    "\tout = {}\n",
    "\tfor i, j in enumerate(range(induce_size)):\n",
    "\t\tout[str(i+1)] = {\n",
    "\t\t\t'input': '{}'.format(examples[i]['input']),\n",
    "\t\t\t'output': '{}'.format(examples[i]['target'] if i < induce_size - noisy_size else examples[noisy_indices[i-induce_size+noisy_size]]['target']),\n",
    "\t\t}\n",
    "\tdict = {\"metadata\": {\n",
    "\t\t\"num_examples\": len(out),\n",
    "\t\t\"description\": \"AG news classification\",\n",
    "\t\t\"task_prefix\": \"\",\n",
    "\t\t},\n",
    "\t\t\t\"examples\": out\n",
    "\t}\n",
    "\twith open('raw/induce/{}.json'.format(task), 'w') as fp:\n",
    "\t\tjson.dump(dict, fp, indent=4)\n",
    "\t\t\n",
    "\t# Valid\n",
    "\tif valid_size > 0:\n",
    "\t\tout = {}\n",
    "\t\tfor i, j in enumerate(range(valid_size)):\n",
    "\t\t\tout[str(i+1)] = {\n",
    "\t\t\t\t'input': '{}'.format(examples[induce_size+i]['input']),\n",
    "\t\t\t\t'output': '{}'.format(examples[induce_size+i]['target']),\n",
    "\t\t\t}\n",
    "\t\t\t\n",
    "\t\tdict = {\"metadata\": {\n",
    "\t\t\t\"num_examples\": len(out),\n",
    "\t\t\t\"description\": \"AG news classification\",\n",
    "\t\t\t\"task_prefix\": \"\",\n",
    "\t\t\t},\n",
    "\t\t\t\t\"examples\": out\n",
    "\t\t}\n",
    "\t\twith open('raw/valid/{}.json'.format(task), 'w') as fp:\n",
    "\t\t\tjson.dump(dict, fp, indent=4)\n",
    "\t\t\n",
    "\t# Execute\n",
    "\tout = {}\n",
    "\tfor i, j in enumerate(range(test_size)):\n",
    "\t\tout[str(i+1)] = {\n",
    "\t\t\t'input': '{}'.format(examples[induce_size+valid_size+i]['input']),\n",
    "\t\t\t'output': '{}'.format(examples[induce_size+valid_size+i]['target']),\n",
    "\t\t}\n",
    "\t\t\n",
    "\tdict = {\"metadata\": {\n",
    "\t\t\"num_examples\": len(out),\n",
    "\t\t\"description\": \"AG news classification\",\n",
    "\t\t\"task_prefix\": \"\",\n",
    "\t\t},\n",
    "\t\t\t\"examples\": out\n",
    "\t}\n",
    "\twith open('raw/execute/{}.json'.format(task), 'w') as fp:\n",
    "\t\tjson.dump(dict, fp, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SST-5 Reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SST-5 remapping\n",
    "from datasets import load_dataset\n",
    "\n",
    "train = load_dataset(\"SetFit/sst5\", split='train')\n",
    "\n",
    "dataset = {}\n",
    "dataset['train'] = train\n",
    "# test = load_dataset(\"SetFit/sst5\", split='test')\n",
    "\n",
    "# {'text': \"final verdict : you 've seen it all before .\",\n",
    "#  'label': 0,\n",
    "#  'label_text': 'very negative'}\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "task_name='sst5_reverse'\n",
    "\n",
    "induce_size = 100 \n",
    "valid_size = 100 \n",
    "test_size = 100\n",
    "label_map = {0:4, 1:3, 2:2, 3:1, 4:0}\n",
    "label_text = {0: 'very negative', 1: 'negative', 2: 'neutral', 3: 'positive', 4: 'very positive'}\n",
    "\n",
    "noise_ratio = [0, 10, 30, 50, 70, 90]\n",
    "# noise_ratio = [0]\n",
    "\n",
    "for noisy_size in noise_ratio:\n",
    "\ttask = '{}_{}_noisy'.format(task_name, noisy_size)\n",
    "\n",
    "\t# shuffle the examples\n",
    "\tnp.random.seed(0)\n",
    "\tperm = np.random.permutation(len(dataset['train']['text']))\n",
    "\texamples = []\n",
    "\tfor i, j in enumerate(perm[:induce_size+valid_size+test_size]):\n",
    "\t\texamples.append({'input':dataset['train']['text'][j], 'target':label_text[label_map[dataset['train']['label'][j]]]})\n",
    "\tprint(\"End\")\n",
    "\tnp.random.seed(0)\n",
    "\tnoisy_indices = np.random.randint(induce_size-noisy_size, induce_size, size=noisy_size)\n",
    "\t# Induce\n",
    "\tout = {}\n",
    "\tfor i, j in enumerate(range(induce_size)):\n",
    "\t\tout[str(i+1)] = {\n",
    "\t\t\t'input': '{}'.format(examples[i]['input']),\n",
    "\t\t\t'output': '{}'.format(examples[i]['target'] if i < induce_size - noisy_size else examples[noisy_indices[i-induce_size+noisy_size]]['target']),\n",
    "\t\t}\n",
    "\tdict = {\"metadata\": {\n",
    "\t\t\"num_examples\": len(out),\n",
    "\t\t\"description\": \"SST-5 classification reversed label\",\n",
    "\t\t},\n",
    "\t\t\t\"examples\": out\n",
    "\t}\n",
    "\twith open('raw/induce/{}.json'.format(task), 'w') as fp:\n",
    "\t\tjson.dump(dict, fp, indent=4)\n",
    "\t\t\n",
    "\t# Valid\n",
    "\tif valid_size > 0:\n",
    "\t\tout = {}\n",
    "\t\tfor i, j in enumerate(range(valid_size)):\n",
    "\t\t\tout[str(i+1)] = {\n",
    "\t\t\t\t'input': '{}'.format(examples[induce_size+i]['input']),\n",
    "\t\t\t\t'output': '{}'.format(examples[induce_size+i]['target']),\n",
    "\t\t\t}\n",
    "\t\t\t\n",
    "\t\tdict = {\"metadata\": {\n",
    "\t\t\t\"num_examples\": len(out),\n",
    "\t\t\t\"description\": \"SST-5 classification reversed label\",\n",
    "\t\t\t\"task_prefix\": \"\",\n",
    "\t\t\t},\n",
    "\t\t\t\t\"examples\": out\n",
    "\t\t}\n",
    "\t\twith open('raw/valid/{}.json'.format(task), 'w') as fp:\n",
    "\t\t\tjson.dump(dict, fp, indent=4)\n",
    "\t\t\n",
    "\t# Execute\n",
    "\tout = {}\n",
    "\tfor i, j in enumerate(range(test_size)):\n",
    "\t\tout[str(i+1)] = {\n",
    "\t\t\t'input': '{}'.format(examples[induce_size+valid_size+i]['input']),\n",
    "\t\t\t'output': '{}'.format(examples[induce_size+valid_size+i]['target']),\n",
    "\t\t}\n",
    "\t\t\n",
    "\tdict = {\"metadata\": {\n",
    "\t\t\"num_examples\": len(out),\n",
    "\t\t\"description\": \"SST-5 classification reversed label\",\n",
    "\t\t\"task_prefix\": \"\",\n",
    "\t\t},\n",
    "\t\t\t\"examples\": out\n",
    "\t}\n",
    "\twith open('raw/execute/{}.json'.format(task), 'w') as fp:\n",
    "\t\tjson.dump(dict, fp, indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "InstructZero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
